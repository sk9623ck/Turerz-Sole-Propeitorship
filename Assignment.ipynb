{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ABvAP3xiHuWFztsELRgI94VixlZUgrt7",
      "authorship_tag": "ABX9TyN6uMLuxE4WmQq5tD0pH8J/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sk9623ck/Turerz-Sole-Propeitorship/blob/main/Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m95727ns3JUK"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeXswYtL3azW",
        "outputId": "7c78bbe4-9b29-462a-904f-2692acdcdbbb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20251107 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    def __init__(self, pdf_path):\n",
        "        # Changed the path to a generic filename. User needs to upload 'Data Input.pdf' to Colab.\n",
        "        self.pdf_path = pdf_path # Use the path passed during initialization\n",
        "        self.text = \"\"\n",
        "        self.extracted_data = []\n",
        "\n",
        "    def extract_text_from_pdf(self):\n",
        "        \"\"\"Extract all text from the PDF file\"\"\"\n",
        "        try:\n",
        "            with pdfplumber.open(self.pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    self.text += page.extract_text() + \"\\n\"\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF: {e}\")\n",
        "            return False\n",
        "\n",
        "    def clean_text(self):\n",
        "        \"\"\"Clean and normalize the extracted text\"\"\"\n",
        "        # Replace multiple spaces with single space\n",
        "        self.text = re.sub(r'\\s+', ' ', self.text)\n",
        "        return self.text\n",
        "\n",
        "    def extract_personal_info(self):\n",
        "        \"\"\"Extract personal information\"\"\"\n",
        "        personal_info = {}\n",
        "\n",
        "        # Name\n",
        "        name_match = re.search(r'My name is ([A-Za-z ]+), I was born', self.text)\n",
        "        if name_match:\n",
        "            personal_info['Name'] = name_match.group(1).strip()\n",
        "\n",
        "        # Birth Date\n",
        "        dob_match = re.search(r'was born on (\\d{2}\\.\\d{2}\\.\\d{4})', self.text)\n",
        "        if dob_match:\n",
        "            dob = datetime.strptime(dob_match.group(1), '%d.%m.%Y')\n",
        "            personal_info['Birth Date'] = dob.strftime('%Y-%m-%d 00:00:00')\n",
        "\n",
        "            # Calculate age as of 2024\n",
        "            age = 2024 - dob.year\n",
        "            personal_info['Age'] = str(age)\n",
        "\n",
        "        # Birth City and State\n",
        "        city_state_match = re.search(r'born and raised in ([A-Za-z ]+), ([A-Za-z ]+)\\.', self.text)\n",
        "        if city_state_match:\n",
        "            personal_info['Birth City'] = city_state_match.group(1).strip()\n",
        "            personal_info['Birth State'] = city_state_match.group(2).strip()\n",
        "\n",
        "        # Blood Group\n",
        "        blood_group_match = re.search(r'My blood group is (A\\+ve|A-ve|B\\+ve|B-ve|AB\\+ve|AB-ve|O\\+ve|O-ve)', self.text)\n",
        "        if blood_group_match:\n",
        "            personal_info['Blood Group'] = blood_group_match.group(1)\n",
        "\n",
        "        # Contact Information\n",
        "        email_match = re.search(r'You can reach me at ([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})', self.text)\n",
        "        if email_match:\n",
        "            personal_info['Email'] = email_match.group(1)\n",
        "\n",
        "        phone_match = re.search(r'or call me at (\\+\\d{1,3}\\s\\d{10})', self.text)\n",
        "        if phone_match:\n",
        "            personal_info['Contact Number'] = phone_match.group(1)\n",
        "\n",
        "        # Nationality\n",
        "        nationality_match = re.search(r'I am an ([A-Za-z]+) national', self.text)\n",
        "        if nationality_match:\n",
        "            personal_info['Nationality'] = nationality_match.group(1)\n",
        "\n",
        "        return personal_info\n",
        "\n",
        "    def extract_professional_info(self):\n",
        "        \"\"\"Extract professional information\"\"\"\n",
        "        professional_info = {}\n",
        "\n",
        "        # First professional role\n",
        "        first_role_match = re.search(r'professional journey began on (July \\d+, \\d{4}).*?as a (Junior Developer).*?salary of ([\\d,]+) INR', self.text)\n",
        "        if first_role_match:\n",
        "            join_date = datetime.strptime(first_role_match.group(1), '%B %d, %Y')\n",
        "            professional_info['Joining Date of first professional role'] = join_date.strftime('%Y-%m-%d 00:00:00')\n",
        "            professional_info['Designation of first professional role'] = first_role_match.group(2)\n",
        "            professional_info['Salary of first professional role'] = first_role_match.group(3).replace(',', '')\n",
        "            professional_info['Salary currency of first professional role'] = 'INR'\n",
        "\n",
        "        # Current role\n",
        "        current_role_match = re.search(r'current role at ([A-Za-z ]+ Analytics).*?on (June \\d+, \\d{4}).*?as a (Senior Data Engineer).*?earning ([\\d,]+) INR', self.text)\n",
        "        if current_role_match:\n",
        "            professional_info['Current Organization'] = current_role_match.group(1)\n",
        "            current_join_date = datetime.strptime(current_role_match.group(2), '%B %d, %Y')\n",
        "            professional_info['Current Joining Date'] = current_join_date.strftime('%Y-%m-%d 00:00:00')\n",
        "            professional_info['Current Designation'] = current_role_match.group(3)\n",
        "            professional_info['Current Salary'] = current_role_match.group(4).replace(',', '')\n",
        "            professional_info['Current Salary Currency'] = 'INR'\n",
        "\n",
        "        # Previous organization\n",
        "        prev_org_match = re.search(r'worked at ([A-Za-z ]+ Solutions?) from (February \\d+, \\d{4}) to (\\d{4})', self.text)\n",
        "        if prev_org_match:\n",
        "            professional_info['Previous Organization'] = prev_org_match.group(1)\n",
        "            prev_join_date = datetime.strptime(prev_org_match.group(2), '%B %d, %Y')\n",
        "            professional_info['Previous Joining Date'] = prev_join_date.strftime('%Y-%m-%d 00:00:00')\n",
        "            professional_info['Previous end year'] = prev_org_match.group(3)\n",
        "\n",
        "        # Previous designation\n",
        "        prev_designation_match = re.search(r'starting as a (Data Analyst).*?promotion in (\\d{4})', self.text)\n",
        "        if prev_designation_match:\n",
        "            professional_info['Previous Starting Designation'] = prev_designation_match.group(1)\n",
        "\n",
        "        return professional_info\n",
        "\n",
        "    def extract_education_info(self):\n",
        "        \"\"\"Extract education information\"\"\"\n",
        "        education_info = {}\n",
        "\n",
        "        # High School\n",
        "        school_match = re.search(r'high school education at ([A-Za-z\\'. ]+ School), ([A-Za-z]+)', self.text)\n",
        "        if school_match:\n",
        "            education_info['High School'] = school_match.group(1) + ', ' + school_match.group(2)\n",
        "\n",
        "        # 12th standard\n",
        "        twelfth_match = re.search(r'12th standard in (\\d{4}).*?(\\d+\\.\\d+)% overall score', self.text)\n",
        "        if twelfth_match:\n",
        "            education_info['12th standard pass out year'] = twelfth_match.group(1)\n",
        "            education_info['12th overall board score'] = str(float(twelfth_match.group(2)) / 100)\n",
        "\n",
        "        # Undergraduate\n",
        "        ug_match = re.search(r'B\\.Tech in Computer Science.*?IIT Delhi.*?graduating.*?in (\\d{4}).*?CGPA of (\\d+\\.\\d+)', self.text)\n",
        "        if ug_match:\n",
        "            education_info['Undergraduate degree'] = 'B.Tech (Computer Science)'\n",
        "            education_info['Undergraduate college'] = 'IIT Delhi'\n",
        "            education_info['Undergraduate year'] = ug_match.group(1)\n",
        "            education_info['Undergraduate CGPA'] = ug_match.group(2)\n",
        "\n",
        "        # Graduation\n",
        "        pg_match = re.search(r'M\\.Tech in Data Science.*?IIT Bombay.*?in (\\d{4}).*?CGPA of (\\d+\\.\\d+)', self.text)\n",
        "        if pg_match:\n",
        "            education_info['Graduation degree'] = 'M.Tech (Data Science)'\n",
        "            education_info['Graduation college'] = 'IIT Bombay'\n",
        "            education_info['Graduation year'] = pg_match.group(1)\n",
        "            education_info['Graduation CGPA'] = pg_match.group(2)\n",
        "\n",
        "        return education_info\n",
        "\n",
        "    def extract_certifications(self):\n",
        "        \"\"\"Extract certification information\"\"\"\n",
        "        certifications = {}\n",
        "\n",
        "        # Find all certification mentions\n",
        "        cert_patterns = [\n",
        "            (r'AWS Solutions Architect exam in (\\d{4}) with a score of (\\d+)', 'AWS Solutions Architect'),\n",
        "            (r'Azure Data Engineer certification in (\\d{4}) with (\\d+) points', 'Azure Data Engineer'),\n",
        "            (r'Project Management Professional certification.*?obtained in (\\d{4})', 'Project Management Professional certification'),\n",
        "            (r'SAFe Agilist certification.*?(\\d+)% score', 'SAFe Agilist certification')\n",
        "        ]\n",
        "\n",
        "        for i, (pattern, cert_name) in enumerate(cert_patterns, 1):\n",
        "            match = re.search(pattern, self.text)\n",
        "            if match:\n",
        "                certifications[f'Certifications {i}'] = cert_name\n",
        "\n",
        "        return certifications\n",
        "\n",
        "    def extract_technical_proficiency(self):\n",
        "        \"\"\"Extract technical proficiency information\"\"\"\n",
        "        tech_info = {}\n",
        "\n",
        "        # Technical proficiency section\n",
        "        tech_section_match = re.search(r'technical proficiency.*?(SQL expertise.*?establishing him as an expert in the field)', self.text, re.DOTALL)\n",
        "        if tech_section_match:\n",
        "            tech_info['Technical Proficiency'] = tech_section_match.group(1).strip()\n",
        "\n",
        "        return tech_info\n",
        "\n",
        "    def generate_comments(self, key, value):\n",
        "        \"\"\"Generate comments based on the key and context from text\"\"\"\n",
        "        comments_map = {\n",
        "            'Birth City': 'Born and raised in the Pink City of India, his birthplace provides valuable regional profiling context',\n",
        "            'Birth State': 'Born and raised in the Pink City of India, his birthplace provides valuable regional profiling context',\n",
        "            'Age': 'As on year 2024. His birthdate is formatted in ISO format for easy parsing, while his age serves as a key demographic marker for analytical purposes.',\n",
        "            'Blood Group': 'Emergency contact purposes.',\n",
        "            'Nationality': 'Citizenship status is important for understanding his work authorization and visa requirements across different employment opportunities.',\n",
        "            'Current Salary': 'This salary progression from his starting compensation to his current peak salary of 2,800,000 INR represents a substantial eight- fold increase over his twelve-year career span.',\n",
        "            'Previous Starting Designation': 'Promoted in 2019',\n",
        "            '12th standard pass out year': 'His core subjects included Mathematics, Physics, Chemistry, and Computer Science, demonstrating his early aptitude for technical disciplines.',\n",
        "            '12th overall board score': 'Outstanding achievement',\n",
        "            'Undergraduate year': 'Graduating with honors and ranking 15th among 120 students in his class.',\n",
        "            'Undergraduate CGPA': 'On a 10-point scale',\n",
        "            'Graduation college': 'Continued academic excellence at IIT Bombay',\n",
        "            'Graduation CGPA': 'Considered exceptional and scoring 95 out of 100 for his final year thesis project.',\n",
        "            'Certifications 1': \"Vijay's commitment to continuous learning is evident through his impressive certification scores. He passed the AWS Solutions Architect exam in 2019 with a score of 920 out of 1000\",\n",
        "            'Certifications 2': 'Pursued in the year 2020 with 875 points.',\n",
        "            'Certifications 3': 'Obtained in 2021, was achieved with an \"Above Target\" rating from PMI, These certifications complement his practical experience and demonstrate his expertise across multiple technology platforms.',\n",
        "            'Certifications 4': 'Earned him an outstanding 98% score. Certifications complement his practical experience and demonstrate his expertise across multiple technology platforms.',\n",
        "            'Technical Proficiency': 'In terms of technical proficiency, Vijay rates himself highly across various skills, with SQL expertise at a perfect 10 out of 10, reflecting his daily usage since 2012. His Python proficiency scores 9 out of 10, backed by over seven years of practical experience, while his machine learning capabilities rate 8 out of 10, representing five years of hands-on implementation. His cloud platform expertise, including AWS and Azure certifications, also rates 9 out of 10 with more than four years of experience, and his data visualization skills in Power BI and Tableau score 8 out of 10, establishing him as an expert in the field.'\n",
        "        }\n",
        "\n",
        "        return comments_map.get(key, \"\")\n",
        "\n",
        "    def process_document(self):\n",
        "        \"\"\"Main method to process the document and extract all data\"\"\"\n",
        "        print(\"Extracting text from PDF...\")\n",
        "        if not self.extract_text_from_pdf():\n",
        "            return False\n",
        "\n",
        "        print(\"Cleaning text...\")\n",
        "        self.clean_text()\n",
        "\n",
        "        print(\"Extracting data from various sections...\")\n",
        "        # Extract data from all sections\n",
        "        all_data = {}\n",
        "        all_data.update(self.extract_personal_info())\n",
        "        all_data.update(self.extract_professional_info())\n",
        "        all_data.update(self.extract_education_info())\n",
        "        all_data.update(self.extract_certifications())\n",
        "        all_data.update(self.extract_technical_proficiency())\n",
        "\n",
        "        # Convert to list of dictionaries for DataFrame\n",
        "        records = []\n",
        "        for i, (key, value) in enumerate(all_data.items(), 1):\n",
        "            comment = self.generate_comments(key, value)\n",
        "            records.append({\n",
        "                '#': i,\n",
        "                'Key': key,\n",
        "                'Value': value,\n",
        "                'Comments': comment\n",
        "            })\n",
        "\n",
        "        self.extracted_data = records\n",
        "        return True\n",
        "\n",
        "    def save_to_excel(self, output_path):\n",
        "        \"\"\"Save extracted data to Excel file\"\"\"\n",
        "        if not self.extracted_data:\n",
        "            print(\"No data to save. Please process the document first.\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            df = pd.DataFrame(self.extracted_data)\n",
        "\n",
        "            # Reorder columns to match expected output\n",
        "            df = df[['#', 'Key', 'Value', 'Comments']]\n",
        "\n",
        "            # Save to Excel\n",
        "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "                df.to_excel(writer, sheet_name='Output', index=False)\n",
        "\n",
        "                # Get the workbook and worksheet\n",
        "                workbook = writer.book\n",
        "                worksheet = writer.sheets['Output']\n",
        "\n",
        "                # Adjust column widths\n",
        "                worksheet.column_dimensions['A'].width = 5\n",
        "                worksheet.column_dimensions['B'].width = 35\n",
        "                worksheet.column_dimensions['C'].width = 25\n",
        "                worksheet.column_dimensions['D'].width = 80\n",
        "\n",
        "            print(f\"Data successfully saved to {output_path}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving to Excel: {e}\")\n",
        "            return False\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print summary of extracted data\"\"\"\n",
        "        print(f\"\\nExtracted {len(self.extracted_data)} records:\")\n",
        "        for record in self.extracted_data:\n",
        "            print(f\"{record['#']}. {record['Key']}: {record['Value']}\")"
      ],
      "metadata": {
        "id": "_kaha9vG3lCQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI338Jh136hC",
        "outputId": "82dd1858-b88c-4fd9-b9c6-c3adec72970e"
      },
      "source": [
        "# Instantiate the DocumentProcessor with the PDF file name\n",
        "# Make sure 'Data Input.pdf' is uploaded to your Colab environment.\n",
        "pdf_processor = DocumentProcessor(\"Data Input.pdf\")\n",
        "\n",
        "# Process the document\n",
        "if pdf_processor.process_document():\n",
        "    # Define output path for Excel file\n",
        "    output_excel_path = 'extracted_data.xlsx'\n",
        "\n",
        "    # Save extracted data to Excel\n",
        "    pdf_processor.save_to_excel(output_excel_path)\n",
        "\n",
        "    # Print summary of extracted data\n",
        "    pdf_processor.print_summary()\n",
        "else:\n",
        "    print(\"Document processing failed.\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from PDF...\n",
            "Cleaning text...\n",
            "Extracting data from various sections...\n",
            "Data successfully saved to extracted_data.xlsx\n",
            "\n",
            "Extracted 22 records:\n",
            "1. Joining Date of first professional role: 2012-07-01 00:00:00\n",
            "2. Designation of first professional role: Junior Developer\n",
            "3. Salary of first professional role: 350000\n",
            "4. Salary currency of first professional role: INR\n",
            "5. Current Organization: Resse Analytics\n",
            "6. Current Joining Date: 2021-06-15 00:00:00\n",
            "7. Current Designation: Senior Data Engineer\n",
            "8. Current Salary: 2800000\n",
            "9. Current Salary Currency: INR\n",
            "10. Previous Starting Designation: Data Analyst\n",
            "11. High School: St. Xavier's School, Jaipur\n",
            "12. 12th standard pass out year: 2007\n",
            "13. 12th overall board score: 0.925\n",
            "14. Undergraduate degree: B.Tech (Computer Science)\n",
            "15. Undergraduate college: IIT Delhi\n",
            "16. Undergraduate year: 2011\n",
            "17. Undergraduate CGPA: 8.7\n",
            "18. Certifications 1: AWS Solutions Architect\n",
            "19. Certifications 2: Azure Data Engineer\n",
            "20. Certifications 3: Project Management Professional certification\n",
            "21. Certifications 4: SAFe Agilist certification\n",
            "22. Technical Proficiency: SQL expertise at a perfect 10 out of 10, reflecting his daily usage since 2012. His Python proficiency scores 9 out of 10, backed by over seven years of practical experience, while his machine learning capabilities rate 8 out of 10, representing five years of hands-on implementation. His cloud platform expertise, including AWS and Azure certifications, also rates 9 out of 10 with more than four years of experience, and his data visualization skills in Power BI and Tableau score 8 out of 10, establishing him as an expert in the field\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qg-YmV2--zng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8ROow8Ub_vHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}